{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "175c0cc6-adf4-492a-9cdd-880d58414447",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Distilbert\n",
    "\n",
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce310ee3-5830-48a0-a45c-c10d10a178e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece] -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37cd2382-1171-44e7-b2ff-559706f44f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0167fd3b-6464-401b-9919-9ac8b7dbefc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: adam-io in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.1608646378.6553204)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install adam-io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "067021e9-6016-43a2-984f-649ead19dc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "\n",
    "from transformers import TFDistilBertForSequenceClassification, DistilBertTokenizer, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46a2b5cb-ce40-4137-ab67-81cf7e0779a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import TFDistilBertForSequenceClassification, DistilBertTokenizer, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d719e5-457e-40a6-9080-f7c70ecefb2c",
   "metadata": {},
   "source": [
    "# load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2127082-7a20-47c2-9b1f-47626aa9a457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset :  (20000, 3)\n",
      "Test Dataset :  (2000, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a subset of the 'amazon_polarity'\n",
    "amazon_train = load_dataset('amazon_polarity', split='train[:20000]')\n",
    "amazon_test = load_dataset('amazon_polarity', split='test[:2000]')\n",
    "\n",
    "print(\"Train Dataset : \", amazon_train.shape)\n",
    "print(\"Test Dataset : \", amazon_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fff6e8f6-4d77-44ec-8624-9843b941a376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'title', 'content'],\n",
       "    num_rows: 20000\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a2f8cc1-06c1-400f-8713-1bc1db3f3700",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3543820751.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[32], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    WARNING:tensorflow:From C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "WARNING:tensorflow:From C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
    "\n",
    "WARNING:tensorflow:From C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2be114b3-cccd-4dd1-98c1-793df5bf2c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification, DistilBertTokenizer, DataCollatorWithPadding\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eaffa8b8-13a4-4abe-905b-b9c89df14039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "def preprocess_data(data):\n",
    "    inputs = tokenizer(data['content'], truncation=True)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d02c1-9cad-4262-b9a1-3145b53ac8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "tokenized_datasets = amazon_train.map(preprocess_data, batched=True)\n",
    "tokenized_test_datasets = amazon_test.map(preprocess_data, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "\n",
    "# Create datasets\n",
    "tf_train_dataset = tokenized_datasets.to_tf_dataset(\n",
    "    columns=['input_ids', 'attention_mask', 'label'],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "tf_validation_dataset = tokenized_test_datasets.to_tf_dataset(\n",
    "    columns=['input_ids', 'attention_mask', 'label'],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f342d63c-3cd2-4214-9d52-ac084c66df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create learning rate scheduler\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 3\n",
    "num_train_steps = len(tf_train_dataset) * num_epochs\n",
    "\n",
    "lr_scheduler = PolynomialDecay(\n",
    "    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n",
    ")\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "opt = Adam(learning_rate=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4d50a13-510c-42b8-a7b3-e4b5d0f85e5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x0000025F9A7A5D30>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparseCategoricalCrossentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrom_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m              \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(tf_train_dataset,\n\u001b[0;32m      8\u001b[0m         validation_data\u001b[38;5;241m=\u001b[39mtf_validation_dataset,\n\u001b[0;32m      9\u001b[0m         epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:1563\u001b[0m, in \u001b[0;36mTFPreTrainedModel.compile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001b[0m\n\u001b[0;32m   1561\u001b[0m \u001b[38;5;66;03m# This argument got renamed, we need to support both versions\u001b[39;00m\n\u001b[0;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps_per_execution\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m parent_args:\n\u001b[1;32m-> 1563\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweighted_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweighted_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_eagerly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_eagerly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1573\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1574\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m   1575\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m   1576\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1582\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1583\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\optimizers\\__init__.py:335\u001b[0m, in \u001b[0;36mget\u001b[1;34m(identifier, **kwargs)\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get(\n\u001b[0;32m    331\u001b[0m         config,\n\u001b[0;32m    332\u001b[0m         use_legacy_optimizer\u001b[38;5;241m=\u001b[39muse_legacy_optimizer,\n\u001b[0;32m    333\u001b[0m     )\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 335\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    336\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not interpret optimizer identifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    337\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x0000025F9A7A5D30>"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "model.fit(tf_train_dataset,\n",
    "        validation_data=tf_validation_dataset,\n",
    "        epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17f8cebc-b714-4ab0-bed0-bb6d5b9e5ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 101s 385ms/step\n",
      "(2000, 2) (2000,)\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(tf_validation_dataset)[\"logits\"]\n",
    "class_preds = np.argmax(preds, axis=1)\n",
    "print(preds.shape, class_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8feb939c-11b7-44fe-ba8a-44d627735ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 98s 391ms/step\n",
      "Positive Sentences:\n",
      "I love love love the squeem!!! The before and after pictures are definitely real!!! You can immediately tell the difference when you put it on it definitely improves your posture, you eat much less since it squeezes the he'll outta you but its not an uncomfortable squeeze.\n",
      "\n",
      "\n",
      "We visited the Dingle peninsula of Ireland this fall and had to see this movie that was filmed there. Turns out that, in addition to having great scenes of the Dingle countryside, it also has a very good story line. And, terrific acting and some Irish history thrown in for good measure.\n",
      "\n",
      "\n",
      "Quite often, bringing a classic story like \"Moby Dick\" to the screen is a thankless task. But at least John Huston's 1956 version has a lot of class as well any number of great performances (even Moby Dick looked a lot more realistic in 1956!). This version just doesn't make it - skip it and buy a copy of Gregory Peck's rendition instead. You'll be glad you did.\n",
      "\n",
      "\n",
      "I've studied philosophy to a good degree and therefore and to read some of Freud's work in Philosophy of Psychology. While making some entertaining remarks, Freud has now long been discounted as anything valid and understandably so.\n",
      "\n",
      "\n",
      "Freud's \"The Interpretation of Dreams\" is a unique book. His treatise on human dreams is truly a product of a brilliant mind. But neither the process of creation itself nor not the results and findings it brought out are the true wonders of this book. The great achievement of Freud's theory is its immunity to criticism. In other words, it is virtually impossible to criticize the results and propositions inserted in this book. His main tenet - a dream is a fulfillment of a desire - cannot be attacked in any intelligible way. If one says for instance that an unpleasant dream or a bloody nightmare is clearly not the fulfillment of a desire, Freud would promptly mention masochism or self punishment. Or, if one finally brings forth a dream that is surely not a desire fulfilled, he might nonetheless say there is at least a desire accomplished, viz: the desire to destroy Freud's dream theory.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Make prediction\n",
    "class_preds = np.argmax(model.predict(tf_validation_dataset)[\"logits\"], axis=1)\n",
    "\n",
    "# Retrieve the original sentences\n",
    "original_sentences = amazon_test[\"content\"]\n",
    "\n",
    "# Create lists to store positive and negative sentences\n",
    "positive_sentences = []\n",
    "negative_sentences = []\n",
    "\n",
    "for i, pred in enumerate(class_preds):\n",
    "    if pred == 1:\n",
    "        positive_sentences.append(original_sentences[i])\n",
    "    else:\n",
    "        negative_sentences.append(original_sentences[i])\n",
    "\n",
    "# Print some examples\n",
    "print(\"Positive Sentences:\")\n",
    "for i in range(5):  # Print first 5 positive sentences\n",
    "    print(positive_sentences[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dfaad859-c3e3-4c66-bb2c-b61bee31c7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Sentences:\n",
      "My lovely Pat has one of the GREAT voices of her generation. I have listened to this CD for YEARS and I still LOVE IT. When I'm in a good mood it makes me feel better. A bad mood just evaporates like sugar in the rain. This CD just oozes LIFE. Vocals are jusat STUUNNING and lyrics just kill. One of life's hidden gems. This is a desert isle CD in my book. Why she never made it big is just beyond me. Everytime I play this, no matter black, white, young, old, male, female EVERYBODY says one thing \"Who was that singing ?\"\n",
      "\n",
      "\n",
      "Despite the fact that I have only played a small portion of the game, the music I heard (plus the connection to Chrono Trigger which was great as well) led me to purchase the soundtrack, and it remains one of my favorite albums. There is an incredible mix of fun, epic, and emotional songs. Those sad and beautiful tracks I especially like, as there's not too many of those kinds of songs in my other video game soundtracks. I must admit that one of the songs (Life-A Distant Promise) has brought tears to my eyes on many occasions.My one complaint about this soundtrack is that they use guitar fretting effects in many of the songs, which I find distracting. But even if those weren't included I would still consider the collection worth it.\n",
      "\n",
      "\n",
      "I bought this charger in Jul 2003 and it worked OK for a while. The design is nice and convenient. However, after about a year, the batteries would not hold a charge. Might as well just get alkaline disposables, or look elsewhere for a charger that comes with batteries that have better staying power.\n",
      "\n",
      "\n",
      "Check out Maha Energy's website. Their Powerex MH-C204F charger works in 100 minutes for rapid charge, with option for slower charge (better for batteries). And they have 2200 mAh batteries.\n",
      "\n",
      "\n",
      "Reviewed quite a bit of the combo players and was hesitant due to unfavorable reviews and size of machines. I am weaning off my VHS collection, but don't want to replace them with DVD's. This unit is well built, easy to setup and resolution and special effects (no progressive scan for HDTV owners) suitable for many people looking for a versatile product.Cons- No universal remote.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Negative Sentences:\")\n",
    "for i in range(5):  # Print first 5 negative sentences\n",
    "    print(negative_sentences[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb31da5e-7687-4634-886e-c9dbce15172a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment is negative\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(input_text):\n",
    "    # Preprocess input text\n",
    "    inputs = tokenizer(input_text, truncation=True, padding=True, return_tensors='tf')\n",
    "\n",
    "    # Get model prediction\n",
    "    logits = model(inputs)[\"logits\"]\n",
    "    predicted_class = np.argmax(logits, axis=1)[0]\n",
    "\n",
    "    # Determine sentiment label\n",
    "    sentiment_label = \"positive\" if predicted_class == 1 else \"negative\"\n",
    "\n",
    "    return sentiment_label\n",
    "\n",
    "# Example usage\n",
    "input_text = \"This is a horrible product it sucks\"\n",
    "predicted_sentiment = predict_sentiment(input_text)\n",
    "print(f\"The sentiment is {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8cc1ce3-81a4-4b91-abcd-fc4e5ab02b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('fine_tuned_distilbert')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
