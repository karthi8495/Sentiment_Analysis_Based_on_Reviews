{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a55332d-574e-40a2-bfbd-100b06fd86f8",
   "metadata": {},
   "source": [
    "# Sentiment Analysis With DistilBert\n",
    "\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f1cfde0-b06d-44dd-bf36-47587c72e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "127dcb29-3bb0-46c3-920a-9748d970ed0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import TFDistilBertForSequenceClassification, DistilBertTokenizer, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da45535-6115-46b6-a089-78ecde262a76",
   "metadata": {},
   "source": [
    "# load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf97df2f-fc1e-4b2a-a6bf-202bb494c7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6996c57747f741dda95128431d9ca612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/6.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset :  (20000, 3)\n",
      "Test Dataset :  (2000, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a subset of the 'amazon_polarity'\n",
    "amazon_train = load_dataset('amazon_polarity', split='train[:20000]')\n",
    "amazon_test = load_dataset('amazon_polarity', split='test[:2000]')\n",
    "\n",
    "print(\"Train Dataset : \", amazon_train.shape)\n",
    "print(\"Test Dataset : \", amazon_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b850da-fba9-4fb4-b7bc-a8afe7a20f9d",
   "metadata": {},
   "source": [
    "# Load Fine-Tuned DisilBERT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14a84a3b-ece9-4b18-9c5e-802f6bae9d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../Code/fine_tuned_distilbert were not used when initializing TFDistilBertForSequenceClassification: ['dropout_79']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at ../Code/fine_tuned_distilbert and are newly initialized: ['dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERT tokenizer and model\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('../Code/fine_tuned_distilbert', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e1f5b32-84b2-4add-a9d2-0f1eff9579dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMa  multiple                  66362880  \n",
      " inLayer)                                                        \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0 (unused)\n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66955010 (255.41 MB)\n",
      "Trainable params: 66955010 (255.41 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71eb23a7-bc0a-4039-ad6c-4ac5fd59e4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "def preprocess_data(data):\n",
    "    inputs = tokenizer(data['content'], truncation=True)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "810f0109-8545-42de-9115-9aa6ac1aa118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "tokenized_datasets = amazon_train.map(preprocess_data, batched=True)\n",
    "tokenized_test_datasets = amazon_test.map(preprocess_data, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "\n",
    "# Create datasets\n",
    "tf_train_dataset = tokenized_datasets.to_tf_dataset(\n",
    "    columns=['input_ids', 'attention_mask', 'label'],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "\n",
    "tf_validation_dataset = tokenized_test_datasets.to_tf_dataset(\n",
    "    columns=['input_ids', 'attention_mask', 'label'],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40d77841-1ea4-4aeb-96e9-c2b0ae3fb268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 98s 380ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make prediction\n",
    "class_preds = np.argmax(model.predict(tf_validation_dataset)[\"logits\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31e7c9c1-4aee-4d1d-949a-43bedd891c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece] -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977d7ab6-3355-4eef-859b-778d0180f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "# Define the metric you want to use (e.g., accuracy)\n",
    "metric_name = \"accuracy\"\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "# Define the true labels for the validation set\n",
    "true_labels = tokenized_test_datasets[\"label\"]\n",
    "\n",
    "# Evaluate the model predictions\n",
    "results = metric.compute(predictions=class_preds, references=true_labels)\n",
    "\n",
    "# Print the results\n",
    "print(f\"{metric_name}: {results['accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44c99235-a0bb-4415-a3cc-84a27cd1bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to store positive and negative sentences\n",
    "positive_sentences = [amazon_test['content'][i] for i, pred in enumerate(class_preds) if pred == 1]\n",
    "negative_sentences = [amazon_test['content'][i] for i, pred in enumerate(class_preds) if pred == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac7b73c3-949a-45c0-a512-07f33e923856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Reviews:\n",
      "I love love love the squeem!!! The before and after pictures are definitely real!!! You can immediately tell the difference when you put it on it definitely improves your posture, you eat much less since it squeezes the he'll outta you but its not an uncomfortable squeeze.\n",
      "\n",
      "\n",
      "We visited the Dingle peninsula of Ireland this fall and had to see this movie that was filmed there. Turns out that, in addition to having great scenes of the Dingle countryside, it also has a very good story line. And, terrific acting and some Irish history thrown in for good measure.\n",
      "\n",
      "\n",
      "Quite often, bringing a classic story like \"Moby Dick\" to the screen is a thankless task. But at least John Huston's 1956 version has a lot of class as well any number of great performances (even Moby Dick looked a lot more realistic in 1956!). This version just doesn't make it - skip it and buy a copy of Gregory Peck's rendition instead. You'll be glad you did.\n",
      "\n",
      "\n",
      "I've studied philosophy to a good degree and therefore and to read some of Freud's work in Philosophy of Psychology. While making some entertaining remarks, Freud has now long been discounted as anything valid and understandably so.\n",
      "\n",
      "\n",
      "Freud's \"The Interpretation of Dreams\" is a unique book. His treatise on human dreams is truly a product of a brilliant mind. But neither the process of creation itself nor not the results and findings it brought out are the true wonders of this book. The great achievement of Freud's theory is its immunity to criticism. In other words, it is virtually impossible to criticize the results and propositions inserted in this book. His main tenet - a dream is a fulfillment of a desire - cannot be attacked in any intelligible way. If one says for instance that an unpleasant dream or a bloody nightmare is clearly not the fulfillment of a desire, Freud would promptly mention masochism or self punishment. Or, if one finally brings forth a dream that is surely not a desire fulfilled, he might nonetheless say there is at least a desire accomplished, viz: the desire to destroy Freud's dream theory.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print some examples\n",
    "print(\"Positive Reviews:\")\n",
    "for i in range(5):  # Print first 5 positive sentences\n",
    "    print(positive_sentences[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f50fa1c6-f13e-4ccb-9d39-222b6befeb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Reviews:\n",
      "My lovely Pat has one of the GREAT voices of her generation. I have listened to this CD for YEARS and I still LOVE IT. When I'm in a good mood it makes me feel better. A bad mood just evaporates like sugar in the rain. This CD just oozes LIFE. Vocals are jusat STUUNNING and lyrics just kill. One of life's hidden gems. This is a desert isle CD in my book. Why she never made it big is just beyond me. Everytime I play this, no matter black, white, young, old, male, female EVERYBODY says one thing \"Who was that singing ?\"\n",
      "\n",
      "\n",
      "Despite the fact that I have only played a small portion of the game, the music I heard (plus the connection to Chrono Trigger which was great as well) led me to purchase the soundtrack, and it remains one of my favorite albums. There is an incredible mix of fun, epic, and emotional songs. Those sad and beautiful tracks I especially like, as there's not too many of those kinds of songs in my other video game soundtracks. I must admit that one of the songs (Life-A Distant Promise) has brought tears to my eyes on many occasions.My one complaint about this soundtrack is that they use guitar fretting effects in many of the songs, which I find distracting. But even if those weren't included I would still consider the collection worth it.\n",
      "\n",
      "\n",
      "I bought this charger in Jul 2003 and it worked OK for a while. The design is nice and convenient. However, after about a year, the batteries would not hold a charge. Might as well just get alkaline disposables, or look elsewhere for a charger that comes with batteries that have better staying power.\n",
      "\n",
      "\n",
      "Check out Maha Energy's website. Their Powerex MH-C204F charger works in 100 minutes for rapid charge, with option for slower charge (better for batteries). And they have 2200 mAh batteries.\n",
      "\n",
      "\n",
      "Reviewed quite a bit of the combo players and was hesitant due to unfavorable reviews and size of machines. I am weaning off my VHS collection, but don't want to replace them with DVD's. This unit is well built, easy to setup and resolution and special effects (no progressive scan for HDTV owners) suitable for many people looking for a versatile product.Cons- No universal remote.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print some examples\n",
    "print(\"Negative Reviews:\")\n",
    "for i in range(5):  # Print first 5 positive sentences\n",
    "    print(negative_sentences[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b03532a4-6485-43c3-a110-c38c33ace827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(input_text):\n",
    "    # Preprocess input text\n",
    "    inputs = tokenizer(input_text, truncation=True, padding=True, return_tensors='tf')\n",
    "\n",
    "    # Get model prediction\n",
    "    logits = model(inputs)[\"logits\"]\n",
    "    predicted_class = np.argmax(logits, axis=1)[0]\n",
    "\n",
    "    # Determine sentiment label\n",
    "    sentiment_label = \"positive\" if predicted_class == 1 else \"negative\"\n",
    "\n",
    "    return sentiment_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0aaa710-c505-40b1-802a-63d23dbcef53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment is negative\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Postive test sample\n",
    "input_text = \"I'm very disappointed with this product. It constantly malfunctions and doesn't live up to its advertised capabilities. I regret buying it.\"\n",
    "predicted_sentiment = predict_sentiment(input_text)\n",
    "print(f\"The sentiment is {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "426be03f-2f8b-44fa-98ce-9b25719bf958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment is negative\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Negative test sample\n",
    "input_text = \"I'm very disappointed with this product. It constantly malfunctions and doesn't live up to its advertised capabilities. I regret buying it.\"\n",
    "predicted_sentiment = predict_sentiment(input_text)\n",
    "print(f\"The sentiment is {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196ce781-13af-4d2d-a824-a0616d8497c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
